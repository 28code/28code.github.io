<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes集群伸缩功能]]></title>
    <url>%2F2019%2F03%2F04%2Fkubernetes_rc2%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes集群扩展功能，这一篇文件带你来 5分钟带你体验kubernetes集群伸缩功能 环境准备本文中我用到了docker的的镜像文件 28code/httpd:v1.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。 启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称，用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes 集群伸缩上次在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，扩展为5个服务 123456789101112扩展命令为# kubectl scale --replicas=5 deployment myappdeployment.extensions/myapp scaled查看pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 57s 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 96m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-kfqpw 1/1 Running 0 57s 10.244.1.26 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 96m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-xxxb9 1/1 Running 0 57s 10.244.2.31 node02.k8s.com &lt;none&gt; &lt;none&gt; 可以看到kubernetes现在有5个pod提供httpd服务。如果我们现在想要伸缩，只需要3个pod提供httpd服务，只需执行命令 123456789101112131415161718# kubectl scale --replicas=3 deployment myappdeployment.extensions/myapp scaled查看pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 17m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 113m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-kfqpw 0/1 Terminating 0 17m 10.244.1.26 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 113m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-xxxb9 0/1 Terminating 0 17m 10.244.2.31 node02.k8s.com &lt;none&gt; &lt;none&gt;可以看到kubernetes结束了2个pod，等候几秒再次查看pod运行情况，pod已经变成3个# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 18m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 113m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 113m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt; 在master节点分别访问3个pod节点查看httpd服务信息123456789# curl 10.244.1.25V1myapp-799bbcd6b4-6697m# curl 10.244.2.30V1myapp-799bbcd6b4-6tqkk# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz 可以看到3个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。 此时我们来测试kubernetes LB功能1234567在master节点执行命令，创建service实现LB访问httpd服务# kubectl expose deployment myapp --name=myapp --port=80查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，123456789101112131415# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk 可以看到访问服务地址10.100.210.72后，分别均匀的访问到3个pod上的httpd服务 以上是体验kubernetes集群伸缩功能整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes集群扩展功能]]></title>
    <url>%2F2019%2F03%2F03%2Fkubernetes_rc%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes LB(Load Balancing)负载均衡集群，这一篇文件带你来 5分钟带你体验kubernetes集群扩展功能 环境准备本文中我用到了docker的的镜像文件 28code/httpd:v1.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。 启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称，用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes 集群扩展上次在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，部署为2个服务1# kubectl run myapp --image=28code/httpd:v1.0 --replicas=2 在master节点执行命令，查看pod的运行情况123456789# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6tqkk 1/1 Running 0 22s 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 22s 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;在master节点执行命令，查看deployment# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEmyapp 2/2 2 2 5m57s 可以看到httpd服务启动了两个分别运行在node01节点上和node02节点上。 此时如果我们想要运行5个httpd服务，此时我们只需要在在master节点执行命令 1234567891011# kubectl scale --replicas=5 deployment myappdeployment.extensions/myapp scaled执行命令后，再次查看pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 57s 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 96m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-kfqpw 1/1 Running 0 57s 10.244.1.26 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 96m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-xxxb9 1/1 Running 0 57s 10.244.2.31 node02.k8s.com &lt;none&gt; &lt;none&gt; 可以看到kubernetes扩展了3个pod提供httpd服务。 在master节点分别访问5个pod节点查看httpd服务信息123456789101112131415# curl 10.244.1.25V1myapp-799bbcd6b4-6697m# curl 10.244.2.30V1myapp-799bbcd6b4-6tqkk# curl 10.244.1.26V1myapp-799bbcd6b4-kfqpw# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz# curl 10.244.2.31V1myapp-799bbcd6b4-xxxb9 可以看到5个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。 此时我们来测试kubernetes LB功能1234567在master节点执行命令，创建service实现LB访问httpd服务# kubectl expose deployment myapp --name=myapp --port=80查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，12345678910111213141516171819202122232425262728293031323334353637383940414243# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-kfqpw# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-xxxb9# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-xxxb9# 可以看到访问服务地址10.100.210.72后，分别均匀的访问到5个pod上的httpd服务 以上是体验kubernetes集群扩展功能整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes LB(Load Balancing)负载均衡集群]]></title>
    <url>%2F2019%2F03%2F02%2Fkubernetes_lb%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes 13.3.4 安装过程，这一篇文件带你来5分钟带你体验kubernetes LB(Load Balancing)负载均衡集群 环境准备本文中我用到了docker的的镜像文件 28code/httpd:v1.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。 启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称，用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes LB负载均衡集群实验在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，部署为2个服务1# kubectl run myapp --image=28code/httpd:v1.0 --replicas=2 在master节点执行命令，查看pod的运行情况123456789# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6tqkk 1/1 Running 0 22s 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 22s 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;在master节点执行命令，查看deployment# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEmyapp 2/2 2 2 5m57s 可以看到httpd服务启动了两个分别运行在node01节点上和node02节点上， 在master节点分别访问两个pod节点查看httpd服务信息123456# curl 10.244.2.30V1myapp-799bbcd6b4-6tqkk# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz 可以看到两个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。 此时我们来测试kubernetes LB功能1234567在master节点执行命令，创建service实现LB访问httpd服务# kubectl expose deployment myapp --name=myapp --port=80查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，123456789101112131415[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz 可以看到访问服务地址10.100.210.72后，分别均匀的访问到2个pod上的httpd服务 以上是体验kubernetes LB(Load Balancing)负载均衡集群整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 13.3.4 安装过程]]></title>
    <url>%2F2019%2F03%2F01%2Fkubernetes_install%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 1.概述今天我们要搭建一个kubernetes环境，主要使用kubeadm工具来搭建。 2.kubeadm简介kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具。点击查看Kubernetes发布的版本 3.操作系统准备3.1. 硬件环境我们使用VM虚拟机来安装一个master节点两个node节点。虚拟机安装CentOS Linux7.6 配置如下：内存：1G （测试1G没问题）CPU：2个 （必须大于等于2）磁盘：20G 3.2. 系统环境1234# cat /etc/redhat-releaseRed Hat Enterprise Linux Server release 7.6 (Maipo)# uname -r3.10.0-957.el7.x86_64 （内核必须是3.10及以上） 4.安装步骤4.1. 网络同步时间确保集群内时钟同步1234# systemctl start chronyd.service# systemctl enable chronyd.service# systemctl status chronyd# systemctl restart chronyd 4.2. 主机名称解析12345# echo &apos;127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4&apos; &gt; /etc/hosts # echo &apos;::1 localhost localhost.localdomain localhost6 localhost6.localdomain6&apos; &gt;&gt; /etc/hosts # echo &apos;192.168.110.140 master.k8s.com master&apos; &gt;&gt; /etc/hosts# echo &apos;192.168.110.141 node01.k8s.com node01&apos; &gt;&gt; /etc/hosts# echo &apos;192.168.110.142 node02.k8s.com node02&apos; &gt;&gt; /etc/hosts 4.3. 关闭iptables1# systemctl status iptables 4.4. 关闭防火墙：1# systemctl status firewalld 4.5. 关闭selinux1# getenforce 编辑 /etc/selinux/config 文件设置 SELINUX=disabled 4.6. 关闭 swap(也可以不关闭，需要在后续改配置文件)12# free -m# swapoff -a 编辑 /etc/fstab 文件 下注释掉所有swap 4.7. 检查net.bridge设置123456789101112# sysctl -a | grep bridgenet.bridge.bridge-nf-call-arptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-filter-pppoe-tagged = 0net.bridge.bridge-nf-filter-vlan-tagged = 0net.bridge.bridge-nf-pass-vlan-input-dev = 0sysctl: reading key &quot;net.ipv6.conf.all.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.default.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.docker0.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.ens33.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.lo.stable_secret&quot; 检查net.bridge.bridge-nf-call-arptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1是否为1 如果不是1修改方式：修改 /etc/sysctl.conf 文件或者新建 /etc/sysctl.d/k8s.conf 增加net.bridge.bridge-nf-call-arptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1让文件生效sysctl -p /etc/sysctl.d/k8s.conf 4.8. yum仓库准备1234567891011121314151617181920212223CentOS-Base的yum仓库# cd /etc/yum.repos.d# mv CentOS-Base.repo CentOS-Base.repo.bak# curl -o CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# sed -i &apos;s/gpgcheck=1/gpgcheck=0/g&apos; /etc/yum.repos.d/CentOS-Base.repodocker-ce的yum仓库# curl -o docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repokubernetes的yum仓库# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgenable=1EOF# update cacheyum clean all yum makecache yum repolist 4.8. 安装Docker12345678安装docker（默认安装最新版本，也可以安装指定版本）# yum -y install docker-ce启动docker# systemctl start docker开机启动docker# systemctl enable docker查看docker信息# docker info 4.9. 安装kubeadm和kubelet和kubectl12安装kubelet kubeadm kubectl（默认安装最新版本，目前版本是1.13.4）# yum install -y kubelet kubeadm kubectl 4.10. 没有关闭swap需要修改参数swap启动的时候 设置防止报错修改 /etc/sysconfig/kubelet 文件KUBELET_EXTRA_ARGS=”–fail-swap-on=false” 4.11. 安装网络插件（flannel或calico）这里我选择安装flannelflannel 默认的设置是:10.244.0.0/16flannel安装方法12执行以下语句# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 点击查看flannel的相关信息 以上是安装好了一个虚拟机，我们需要把虚拟机复制2份，总共3个虚拟机。设置3个虚拟机的主机名和ip分别为(可根据实际自行设定) 主机名 ip master 192.168.110.140 node01 192.168.110.141 node02 192.168.110.142 5.镜像准备 查看master 节点上需要的镜像文件12345678# kubeadm config images listk8s.gcr.io/kube-apiserver:v1.13.4k8s.gcr.io/kube-controller-manager:v1.13.4k8s.gcr.io/kube-scheduler:v1.13.4k8s.gcr.io/kube-proxy:v1.13.4k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.2.24k8s.gcr.io/coredns:1.2.6 为了解决国内普遍访问不到k8s.gcr.io的问题，我们从mirrorgooglecontainers下载image，然后打个tag来绕过网络限制： 123456789101112131415docker pull mirrorgooglecontainers/kube-apiserver:v1.13.4docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.4docker pull mirrorgooglecontainers/kube-scheduler:v1.13.4docker pull mirrorgooglecontainers/kube-proxy:v1.13.4docker pull mirrorgooglecontainers/pause:3.1docker pull mirrorgooglecontainers/etcd:3.2.24docker pull mirrorgooglecontainers/coredns:1.2.6docker tag mirrorgooglecontainers/kube-apiserver:v1.13.4 k8s.gcr.io/kube-apiserver:v1.13.4docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.4 k8s.gcr.io/kube-controller-manager:v1.13.4docker tag mirrorgooglecontainers/kube-scheduler:v1.13.4 k8s.gcr.io/kube-scheduler:v1.13.4docker tag mirrorgooglecontainers/kube-proxy:v1.13.4 k8s.gcr.io/kube-proxy:v1.13.4docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24docker tag mirrorgooglecontainers/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6 6. kubernetes master初始化123456查看kubeadm版本# rpm -q kubeadmkubeadm-1.13.4-0.x86_64执行命令初始化master# kubeadm init --kubernetes-version=&quot;1.13.4&quot; --pod-network-cidr=&quot;10.244.0.0/16&quot; --ignore-preflight-errors=Swap 初始化信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[init] Using Kubernetes version: v1.13.4[preflight] Running pre-flight checks [WARNING Swap]: running with swap on is not supported. Please disable swap [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06 [WARNING Service-Kubelet]: kubelet service is not enabled, please run &apos;systemctl enable kubelet.service&apos;[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;[certs] Generating &quot;front-proxy-ca&quot; certificate and key[certs] Generating &quot;front-proxy-client&quot; certificate and key[certs] Generating &quot;etcd/ca&quot; certificate and key[certs] Generating &quot;etcd/server&quot; certificate and key[certs] etcd/server serving cert is signed for DNS names [master.k8s.com localhost] and IPs [192.168.110.140 127.0.0.1 ::1][certs] Generating &quot;etcd/peer&quot; certificate and key[certs] etcd/peer serving cert is signed for DNS names [master.k8s.com localhost] and IPs [192.168.110.140 127.0.0.1 ::1][certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key[certs] Generating &quot;ca&quot; certificate and key[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key[certs] Generating &quot;apiserver&quot; certificate and key[certs] apiserver serving cert is signed for DNS names [master.k8s.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.110.140][certs] Generating &quot;sa&quot; key and public key[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s[apiclient] All control plane components are healthy after 24.503251 seconds[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.13&quot; in namespace kube-system with the configuration for the kubelets in the cluster[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;master.k8s.com&quot; as an annotation[mark-control-plane] Marking the node master.k8s.com as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;[mark-control-plane] Marking the node master.k8s.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: 0me0lv.xn1vhr6ub80qexyu[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.110.140:6443 --token 0me0lv.xn1vhr6ub80qexyu --discovery-token-ca-cert-hash sha256:700e4cb4ce01c0be5a40e8278ebf531847090b53d53b115ca3031bfb25532b0b 根据提示在master节点下执行以下命令123# mkdir -p $HOME/.kube# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config# sudo chown $(id -u):$(id -g) $HOME/.kube/config 7. kubernetes node01加入集群node01执行以下命令加入集群1#kubeadm join 192.168.110.140:6443 --token 0me0lv.xn1vhr6ub80qexyu --discovery-token-ca-cert-hash sha256:700e4cb4ce01c0be5a40e8278ebf531847090b53d53b115ca3031bfb25532b0b --ignore-preflight-errors=Swap 执行命令后提示信息1234567891011121314151617181920212223[preflight] Running pre-flight checks [WARNING Swap]: running with swap on is not supported. Please disable swap [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06 [WARNING Service-Kubelet]: kubelet service is not enabled, please run &apos;systemctl enable kubelet.service&apos;[discovery] Trying to connect to API Server &quot;192.168.110.140:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://192.168.110.140:6443&quot;[discovery] Requesting info from &quot;https://192.168.110.140:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;192.168.110.140:6443&quot;[discovery] Successfully established connection with API Server &quot;192.168.110.140:6443&quot;[join] Reading configuration from the cluster...[join] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.13&quot; ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Activating the kubelet service[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;node01.k8s.com&quot; as an annotationThis node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster. 8. kubernetes node02加入集群node02执行以下命令加入集群1#kubeadm join 192.168.110.140:6443 --token 0me0lv.xn1vhr6ub80qexyu --discovery-token-ca-cert-hash sha256:700e4cb4ce01c0be5a40e8278ebf531847090b53d53b115ca3031bfb25532b0b --ignore-preflight-errors=Swap 以上是配置好了kubernetes 一个master节点和两个node节点 9. 验证集群状态分别在3个节点上执行命令启动kubelet1# system start kubelet 在master节点上执行命令查看集群节点状态123456# kubectl get nodes得到以下信息NAME STATUS ROLES AGE VERSIONmaster.k8s.com Ready master 18m v1.13.4node01.k8s.com Ready &lt;none&gt; 11m v1.13.4node02.k8s.com Ready &lt;none&gt; 9m39s v1.13.4 在master节点上执行命令查看系统pod运行状态123456789101112131415# kubectl get pods -n kube-system得到以下信息NAME READY STATUS RESTARTS AGEcoredns-86c58d9df4-467w8 1/1 Running 0 23mcoredns-86c58d9df4-qrjx2 1/1 Running 0 23metcd-master.k8s.com 1/1 Running 0 22mkube-apiserver-master.k8s.com 1/1 Running 0 22mkube-controller-manager-master.k8s.com 1/1 Running 0 22mkube-flannel-ds-amd64-4hx5q 1/1 Running 0 5m1skube-flannel-ds-amd64-nhbqs 1/1 Running 0 5m1skube-flannel-ds-amd64-pf5dg 1/1 Running 0 5m1skube-proxy-g85lj 1/1 Running 0 14mkube-proxy-kb9l7 1/1 Running 0 23mkube-proxy-nczpr 1/1 Running 0 16mkube-scheduler-master.k8s.com 1/1 Running 0 22m 9. 错误排查在master节点上执行命令得到如下状态1234567891011121314151617# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster.k8s.com NotReady master 15m v1.13.4node01.k8s.com NotReady &lt;none&gt; 8m6s v1.13.4node02.k8s.com NotReady &lt;none&gt; 6m31s v1.13.4# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-86c58d9df4-467w8 0/1 Pending 0 17mcoredns-86c58d9df4-qrjx2 0/1 Pending 0 17metcd-master.k8s.com 1/1 Running 0 16mkube-apiserver-master.k8s.com 1/1 Running 0 16mkube-controller-manager-master.k8s.com 1/1 Running 0 16mkube-proxy-g85lj 1/1 Running 0 8m19skube-proxy-kb9l7 1/1 Running 0 17mkube-proxy-nczpr 1/1 Running 0 9m54skube-scheduler-master.k8s.com 1/1 Running 0 16m 以上说明网络没有配置好，需要我们安装网络，此时参考 4.11. 安装网络插件（flannel或calico）执行命令安装flannel1# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 执行命令后再次验证节点和pod状态 以上完整安装kubernetes 13.3.4 整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
</search>
