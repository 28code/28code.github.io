<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes 对外提供访问]]></title>
    <url>%2F2019%2F03%2F08%2Fkubernetes_nodeport%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes kubernetes RollingUpdate滚动升级镜像回滚，这一篇文章带你来 5分钟带你体验kubernetes 对外提供访问 环境准备本文是之前环境的延续。 kubernetes 对外提供访问在master我们执行命令查看pod12345# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-8nz5g 1/1 Running 0 91s 10.244.1.30 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-jms4x 1/1 Running 0 92s 10.244.1.29 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-l7q6v 1/1 Running 0 89s 10.244.2.34 node02.k8s.com &lt;none&gt; &lt;none&gt; 在master我们执行命令查看service12345查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 我们想要访问pod，目前只能在虚拟机的节点上访问，10.244.XXX.XXX的网址或者通过service服务的ip访问，如果我们想要在虚拟机之外访问集群内部的地址，这个时候我们需要执行命令修改service的文件 12master上执行命令# kubectl edit svc myapp 得到如下信息123456789101112131415161718192021222324252627# Please edit the object below. Lines beginning with a &apos;#&apos; will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1kind: Servicemetadata: creationTimestamp: &quot;2019-03-07T05:53:57Z&quot; labels: run: myapp name: myapp namespace: default resourceVersion: &quot;5195&quot; selfLink: /api/v1/namespaces/default/services/myapp uid: 38910862-4942-11e9-bbc4-000c296cc22aspec: clusterIP: 10.100.210.72 ports: - port: 80 protocol: TCP targetPort: 80 selector: run: myapp sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; 找到 type: ClusterIP 修改为 type: NodePort修改后结果 123456789101112131415161718192021222324252627# Please edit the object below. Lines beginning with a &apos;#&apos; will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1kind: Servicemetadata: creationTimestamp: &quot;2019-03-18T05:53:57Z&quot; labels: run: myapp name: myapp namespace: default resourceVersion: &quot;5195&quot; selfLink: /api/v1/namespaces/default/services/myapp uid: 38910862-4942-11e9-bbc4-000c296cc22aspec: clusterIP: 10.100.210.72 ports: - port: 80 protocol: TCP targetPort: 80 selector: run: myapp sessionAffinity: None type: NodePortstatus: loadBalancer: &#123;&#125; 保存退出再次查看service1234# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 4h14mmyapp NodePort 10.100.210.72 &lt;none&gt; 80:32313/TCP 3h19m 发现myapp service的prot变为 80:32313/TCP，32313为master主机对外访问的端口号。master的ip为 192.168.110.140，此时我们想要在虚拟机外部访问集群内部pod，需要使用 http://192.168.110.140:32313 。在master节点上执行命令查看效果12345678910# curl http://192.168.110.140:32313V1myapp-799bbcd6b4-l7q6v# curl http://192.168.110.140:32313V1myapp-799bbcd6b4-jms4x# curl http://192.168.110.140:32313V1myapp-799bbcd6b4-8nz5g# curl http://192.168.110.140:32313 同时我在虚拟机的外部机器上访问 http://192.168.110.140:32313 以上是kubernetes 对外提供访问 整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes RollingUpdate滚动升级镜像回滚]]></title>
    <url>%2F2019%2F03%2F07%2Fkubernetes_rub%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes RollingUpdate 滚动升级，这一篇文章带你来 5分钟带你体验kubernetes RollingUpdate滚动升级镜像回滚 环境准备本文中我用到了docker的两个镜像文件 28code/httpd:v1.0 和 28code/httpd:v2.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称,启动28code/httpd:v2.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V2和启动pod时候的pod名称，以上主要用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes RollingUpdate 滚动升级镜像回滚上次在master执行命令，部署docker镜像文件28code/httpd:v2.0，，镜像提供httpd服务，为3个服务。 查看pod运行情况12345# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-6fd9d6f87-54xqz 1/1 Running 0 49s 10.244.1.28 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-6fd9d6f87-7grz9 1/1 Running 0 53s 10.244.2.32 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-6fd9d6f87-s4sqw 1/1 Running 0 51s 10.244.2.33 node02.k8s.com &lt;none&gt; &lt;none&gt; 在master节点上执行访问3个pod，看到返回信息都是V2版本123456789# curl 10.244.1.28V2myapp-6fd9d6f87-54xqz# curl 10.244.2.32V2myapp-6fd9d6f87-7grz9# curl 10.244.2.33V2myapp-6fd9d6f87-s4sqw 现在我们回退到V1版本，我们执行以下命令12# kubectl rollout undo deployment myappdeployment.extensions/myapp rolled back master节点上查看pod信息，看到pod正在回滚12345678# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-6fd9d6f87-54xqz 1/1 Terminating 0 21m 10.244.1.28 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-6fd9d6f87-7grz9 1/1 Terminating 0 21m 10.244.2.32 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-6fd9d6f87-s4sqw 1/1 Terminating 0 21m 10.244.2.33 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-8nz5g 1/1 Running 0 21s 10.244.1.30 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-jms4x 1/1 Running 0 22s 10.244.1.29 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-l7q6v 1/1 Running 0 19s 10.244.2.34 node02.k8s.com &lt;none&gt; &lt;none&gt; 再次master节点上查看pod信息12345# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-8nz5g 1/1 Running 0 91s 10.244.1.30 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-jms4x 1/1 Running 0 92s 10.244.1.29 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-l7q6v 1/1 Running 0 89s 10.244.2.34 node02.k8s.com &lt;none&gt; &lt;none&gt; 在master节点上执行访问3个pod，看到返回信息都是V1版本123456789# curl 10.244.1.30V1myapp-799bbcd6b4-8nz5g# curl 10.244.1.29V1myapp-799bbcd6b4-jms4x# curl 10.244.2.34V1myapp-799bbcd6b4-l7q6v 此时我们来测试kubernetes LB功能12345查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，123456789101112# curl 10.100.210.72V1myapp-799bbcd6b4-jms4x# curl 10.100.210.72V1myapp-799bbcd6b4-8nz5g# curl 10.100.210.72V1myapp-799bbcd6b4-l7q6v# curl 10.100.210.72V1myapp-799bbcd6b4-jms4x 可以看到访问服务地址10.100.210.72后，分别均匀的访问到3个pod上的httpd服务，版本已经回退变成V1版本。 以上是kubernetes RollingUpdate 滚动升级 整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes RollingUpdate 滚动升级]]></title>
    <url>%2F2019%2F03%2F06%2Fkubernetes_ru%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes HA(High Available)高可用集群，这一篇文章带你来 5分钟带你体验kubernetes RollingUpdate 滚动升级 环境准备本文中我用到了docker的两个镜像文件 28code/httpd:v1.0 和 28code/httpd:v2.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称,启动28code/httpd:v2.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V2和启动pod时候的pod名称，以上主要用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes RollingUpdate 滚动升级上次在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，为3个服务。上次我们把node02节点关闭后，这次把node02节点启动起来，并且执行命令启动node02节点上的kubelet 1[root@node02 ~]# systemctl start kubelet 在master上执行命令123456查看目前集群pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 47m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-9cq9p 1/1 Running 0 7m29s 10.244.1.27 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 142m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt; 在master节点分别访问3个pod节点查看httpd服务信息123456789# curl 10.244.1.25V1myapp-799bbcd6b4-6697m# curl 10.244.1.27V1myapp-799bbcd6b4-9cq9p# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz 可以看到3个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。我们所有pod都运行了28code/httpd:v1.0的image，现在我需要把pod都升级到28code/httpd:v2.0，我们在master节点执行以下命令12# kubectl set image deployment myapp myapp=28code/httpd:v2.0deployment.extensions/myapp image updated 再次查看pod运行情况，和上边信息对比发现pod已经都变成新的pod，并且之前都运行在node01上的3个pod，现在已经变成1个pod运行在node01上，2个pod运行在node02上。12345# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-6fd9d6f87-54xqz 1/1 Running 0 49s 10.244.1.28 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-6fd9d6f87-7grz9 1/1 Running 0 53s 10.244.2.32 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-6fd9d6f87-s4sqw 1/1 Running 0 51s 10.244.2.33 node02.k8s.com &lt;none&gt; &lt;none&gt; 在master节点上执行访问3个pod，看到返回信息已经是V2版本123456789# curl 10.244.1.28V2myapp-6fd9d6f87-54xqz# curl 10.244.2.32V2myapp-6fd9d6f87-7grz9# curl 10.244.2.33V2myapp-6fd9d6f87-s4sqw 此时我们来测试kubernetes LB功能123456查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，123456789101112# curl 10.100.210.72V2myapp-6fd9d6f87-7grz9# curl 10.100.210.72V2myapp-6fd9d6f87-s4sqw# curl 10.100.210.72V2myapp-6fd9d6f87-54xqz# curl 10.100.210.72V2myapp-6fd9d6f87-s4sqw 可以看到访问服务地址10.100.210.72后，分别均匀的访问到3个pod上的httpd服务，版本已经变成V2版本。 以上是kubernetes RollingUpdate 滚动升级 整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes HA(High Available)高可用集群]]></title>
    <url>%2F2019%2F03%2F05%2Fkubernetes_ha%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes集群伸缩功能，这一篇文件带你来 5分钟带你体验kubernetes HA(High Available)高可用集群 环境准备本文中我用到了docker的的镜像文件 28code/httpd:v1.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。 启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称，用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes HA(High Available)高可用集群上次在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，为3个服务 123456查看目前集群pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 18m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 113m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 113m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt; 可以看到3个pod有2个pod运行在node01节点上有1个pod运行在node02节点上。此时我们让node02断电，看看会发生什么。在node02节点上我们执行命令，让 node02 关机1[root@node02 ~]# init 0 在master上检查node的情况，发现node02状态已经发生变化12345# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster.k8s.com Ready master 3h3m v1.13.4node01.k8s.com Ready &lt;none&gt; 176m v1.13.4node02.k8s.com NotReady &lt;none&gt; 174m v1.13.4 在master上检查pod的情况（需要等待一段时间），发现运行在node02节点上pod已经被结束，运行的总pod还是3个，都运行在了node01上。123456# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 40m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Terminating 0 135m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-9cq9p 1/1 Running 0 9s 10.244.1.27 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 135m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt; 再等待一段时间后，检测pod状态，发现只剩下3个pod，都运行在了node01上。12345# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 47m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-9cq9p 1/1 Running 0 7m29s 10.244.1.27 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 142m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt; 在master节点分别访问3个pod节点查看httpd服务信息123456789# curl 10.244.1.25V1myapp-799bbcd6b4-6697m# curl 10.244.1.27V1myapp-799bbcd6b4-9cq9p# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz 可以看到3个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。 此时我们来测试kubernetes LB功能123456查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，123456789# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-9cq9p 可以看到访问服务地址10.100.210.72后，分别均匀的访问到3个pod上的httpd服务 以上是kubernetes HA(High Available)高可用集群整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes集群伸缩功能]]></title>
    <url>%2F2019%2F03%2F04%2Fkubernetes_rc2%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes集群扩展功能，这一篇文件带你来 5分钟带你体验kubernetes集群伸缩功能 环境准备本文中我用到了docker的的镜像文件 28code/httpd:v1.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。 启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称，用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes 集群伸缩上次在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，扩展为5个服务 123456789101112扩展命令为# kubectl scale --replicas=5 deployment myappdeployment.extensions/myapp scaled查看pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 57s 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 96m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-kfqpw 1/1 Running 0 57s 10.244.1.26 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 96m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-xxxb9 1/1 Running 0 57s 10.244.2.31 node02.k8s.com &lt;none&gt; &lt;none&gt; 可以看到kubernetes现在有5个pod提供httpd服务。如果我们现在想要伸缩，只需要3个pod提供httpd服务，只需执行命令 123456789101112131415161718# kubectl scale --replicas=3 deployment myappdeployment.extensions/myapp scaled查看pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 17m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 113m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-kfqpw 0/1 Terminating 0 17m 10.244.1.26 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 113m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-xxxb9 0/1 Terminating 0 17m 10.244.2.31 node02.k8s.com &lt;none&gt; &lt;none&gt;可以看到kubernetes结束了2个pod，等候几秒再次查看pod运行情况，pod已经变成3个# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 18m 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 113m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 113m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt; 在master节点分别访问3个pod节点查看httpd服务信息123456789# curl 10.244.1.25V1myapp-799bbcd6b4-6697m# curl 10.244.2.30V1myapp-799bbcd6b4-6tqkk# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz 可以看到3个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。 此时我们来测试kubernetes LB功能123456查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，123456789101112131415# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk 可以看到访问服务地址10.100.210.72后，分别均匀的访问到3个pod上的httpd服务 以上是体验kubernetes集群伸缩功能整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes集群扩展功能]]></title>
    <url>%2F2019%2F03%2F03%2Fkubernetes_rc%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes LB(Load Balancing)负载均衡集群，这一篇文件带你来 5分钟带你体验kubernetes集群扩展功能 环境准备本文中我用到了docker的的镜像文件 28code/httpd:v1.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。 启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称，用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes 集群扩展上次在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，部署为2个服务1# kubectl run myapp --image=28code/httpd:v1.0 --replicas=2 在master节点执行命令，查看pod的运行情况123456789# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6tqkk 1/1 Running 0 22s 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 22s 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;在master节点执行命令，查看deployment# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEmyapp 2/2 2 2 5m57s 可以看到httpd服务启动了两个分别运行在node01节点上和node02节点上。 此时如果我们想要运行5个httpd服务，此时我们只需要在在master节点执行命令 1234567891011# kubectl scale --replicas=5 deployment myappdeployment.extensions/myapp scaled执行命令后，再次查看pod运行情况# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6697m 1/1 Running 0 57s 10.244.1.25 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-6tqkk 1/1 Running 0 96m 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-kfqpw 1/1 Running 0 57s 10.244.1.26 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 96m 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-xxxb9 1/1 Running 0 57s 10.244.2.31 node02.k8s.com &lt;none&gt; &lt;none&gt; 可以看到kubernetes扩展了3个pod提供httpd服务。 在master节点分别访问5个pod节点查看httpd服务信息123456789101112131415# curl 10.244.1.25V1myapp-799bbcd6b4-6697m# curl 10.244.2.30V1myapp-799bbcd6b4-6tqkk# curl 10.244.1.26V1myapp-799bbcd6b4-kfqpw# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz# curl 10.244.2.31V1myapp-799bbcd6b4-xxxb9 可以看到5个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。 此时我们来测试kubernetes LB功能123456查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，12345678910111213141516171819202122232425262728293031323334353637383940414243# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-kfqpw# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-6697m# curl 10.100.210.72V1myapp-799bbcd6b4-xxxb9# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz# curl 10.100.210.72V1myapp-799bbcd6b4-xxxb9# 可以看到访问服务地址10.100.210.72后，分别均匀的访问到5个pod上的httpd服务 以上是体验kubernetes集群扩展功能整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟带你体验kubernetes LB(Load Balancing)负载均衡集群]]></title>
    <url>%2F2019%2F03%2F02%2Fkubernetes_lb%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 上一篇文章介绍了kubernetes 13.3.4 安装过程，这一篇文件带你来5分钟带你体验kubernetes LB(Load Balancing)负载均衡集群 环境准备本文中我用到了docker的的镜像文件 28code/httpd:v1.0 ，系作者基于docker的busybox镜像做的一个docker image，主要功能是提供httpd服务并显示版本和pod信息。 启动28code/httpd:v1.0镜像时候会生成文件位于 /var/www/index.html，index.html记录了版本号V1和启动pod时候的pod名称，用来提供展示版本变更和访问时候显示访问的哪个pod的显示效果。 kubernetes LB负载均衡集群实验在master执行命令，部署docker镜像文件28code/httpd:v1.0，，镜像提供httpd服务，部署为2个服务1# kubectl run myapp --image=28code/httpd:v1.0 --replicas=2 在master节点执行命令，查看pod的运行情况123456789# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-799bbcd6b4-6tqkk 1/1 Running 0 22s 10.244.2.30 node02.k8s.com &lt;none&gt; &lt;none&gt;myapp-799bbcd6b4-p4dxz 1/1 Running 0 22s 10.244.1.24 node01.k8s.com &lt;none&gt; &lt;none&gt;在master节点执行命令，查看deployment# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEmyapp 2/2 2 2 5m57s 可以看到httpd服务启动了两个分别运行在node01节点上和node02节点上， 在master节点分别访问两个pod节点查看httpd服务信息123456# curl 10.244.2.30V1myapp-799bbcd6b4-6tqkk# curl 10.244.1.24V1myapp-799bbcd6b4-p4dxz 可以看到两个pod节点正常访问（V1代表版本信息，myapp-XXXXXX 是代表pods名称），显示了版本号和pod名称。 此时我们来测试kubernetes LB功能1234567在master节点执行命令，创建service实现LB访问httpd服务# kubectl expose deployment myapp --name=myapp --port=80查看 service# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 72mmyapp ClusterIP 10.100.210.72 &lt;none&gt; 80/TCP 17m 在master节点执行命令展示LB的效果，123456789101112131415[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-6tqkk[root@master ~]# curl 10.100.210.72V1myapp-799bbcd6b4-p4dxz 可以看到访问服务地址10.100.210.72后，分别均匀的访问到2个pod上的httpd服务 以上是体验kubernetes LB(Load Balancing)负载均衡集群整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 1.13.4 安装过程]]></title>
    <url>%2F2019%2F03%2F01%2Fkubernetes_install%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 1.概述今天我们要搭建一个kubernetes环境，主要使用kubeadm工具来搭建。 2.kubeadm简介kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具。点击查看Kubernetes发布的版本 3.操作系统准备3.1. 硬件环境我们使用VM虚拟机来安装一个master节点两个node节点。虚拟机安装CentOS Linux7.6 配置如下：内存：1G （测试1G没问题）CPU：2个 （必须大于等于2）磁盘：20G 3.2. 系统环境1234# cat /etc/redhat-releaseRed Hat Enterprise Linux Server release 7.6 (Maipo)# uname -r3.10.0-957.el7.x86_64 （内核必须是3.10及以上） 4.安装步骤4.1. 网络同步时间确保集群内时钟同步1234# systemctl start chronyd.service# systemctl enable chronyd.service# systemctl status chronyd# systemctl restart chronyd 4.2. 主机名称解析12345# echo &apos;127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4&apos; &gt; /etc/hosts # echo &apos;::1 localhost localhost.localdomain localhost6 localhost6.localdomain6&apos; &gt;&gt; /etc/hosts # echo &apos;192.168.110.140 master.k8s.com master&apos; &gt;&gt; /etc/hosts# echo &apos;192.168.110.141 node01.k8s.com node01&apos; &gt;&gt; /etc/hosts# echo &apos;192.168.110.142 node02.k8s.com node02&apos; &gt;&gt; /etc/hosts 4.3. 关闭iptables1# systemctl status iptables 4.4. 关闭防火墙：1# systemctl status firewalld 4.5. 关闭selinux1# getenforce 编辑 /etc/selinux/config 文件设置 SELINUX=disabled 4.6. 关闭 swap(也可以不关闭，需要在后续改配置文件)12# free -m# swapoff -a 编辑 /etc/fstab 文件 下注释掉所有swap 4.7. 检查net.bridge设置123456789101112# sysctl -a | grep bridgenet.bridge.bridge-nf-call-arptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-filter-pppoe-tagged = 0net.bridge.bridge-nf-filter-vlan-tagged = 0net.bridge.bridge-nf-pass-vlan-input-dev = 0sysctl: reading key &quot;net.ipv6.conf.all.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.default.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.docker0.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.ens33.stable_secret&quot;sysctl: reading key &quot;net.ipv6.conf.lo.stable_secret&quot; 检查net.bridge.bridge-nf-call-arptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1是否为1 如果不是1修改方式：修改 /etc/sysctl.conf 文件或者新建 /etc/sysctl.d/k8s.conf 增加net.bridge.bridge-nf-call-arptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1让文件生效sysctl -p /etc/sysctl.d/k8s.conf 4.8. yum仓库准备1234567891011121314151617181920212223CentOS-Base的yum仓库# cd /etc/yum.repos.d# mv CentOS-Base.repo CentOS-Base.repo.bak# curl -o CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# sed -i &apos;s/gpgcheck=1/gpgcheck=0/g&apos; /etc/yum.repos.d/CentOS-Base.repodocker-ce的yum仓库# curl -o docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repokubernetes的yum仓库# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgenable=1EOF# update cacheyum clean all yum makecache yum repolist 4.8. 安装Docker12345678安装docker（默认安装最新版本，也可以安装指定版本）# yum -y install docker-ce启动docker# systemctl start docker开机启动docker# systemctl enable docker查看docker信息# docker info 4.9. 安装kubeadm和kubelet和kubectl12安装kubelet kubeadm kubectl（默认安装最新版本，目前版本是1.13.4）# yum install -y kubelet kubeadm kubectl 4.10. 没有关闭swap需要修改参数swap启动的时候 设置防止报错修改 /etc/sysconfig/kubelet 文件KUBELET_EXTRA_ARGS=”–fail-swap-on=false” 4.11. 安装网络插件（flannel或calico）这里我选择安装flannelflannel 默认的设置是:10.244.0.0/16flannel安装方法12执行以下语句# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 点击查看flannel的相关信息 以上是安装好了一个虚拟机，我们需要把虚拟机复制2份，总共3个虚拟机。设置3个虚拟机的主机名和ip分别为(可根据实际自行设定) 主机名 ip master 192.168.110.140 node01 192.168.110.141 node02 192.168.110.142 5.镜像准备 查看master 节点上需要的镜像文件12345678# kubeadm config images listk8s.gcr.io/kube-apiserver:v1.13.4k8s.gcr.io/kube-controller-manager:v1.13.4k8s.gcr.io/kube-scheduler:v1.13.4k8s.gcr.io/kube-proxy:v1.13.4k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.2.24k8s.gcr.io/coredns:1.2.6 为了解决国内普遍访问不到k8s.gcr.io的问题，我们从mirrorgooglecontainers下载image，然后打个tag来绕过网络限制： 123456789101112131415docker pull mirrorgooglecontainers/kube-apiserver:v1.13.4docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.4docker pull mirrorgooglecontainers/kube-scheduler:v1.13.4docker pull mirrorgooglecontainers/kube-proxy:v1.13.4docker pull mirrorgooglecontainers/pause:3.1docker pull mirrorgooglecontainers/etcd:3.2.24docker pull mirrorgooglecontainers/coredns:1.2.6docker tag mirrorgooglecontainers/kube-apiserver:v1.13.4 k8s.gcr.io/kube-apiserver:v1.13.4docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.4 k8s.gcr.io/kube-controller-manager:v1.13.4docker tag mirrorgooglecontainers/kube-scheduler:v1.13.4 k8s.gcr.io/kube-scheduler:v1.13.4docker tag mirrorgooglecontainers/kube-proxy:v1.13.4 k8s.gcr.io/kube-proxy:v1.13.4docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24docker tag mirrorgooglecontainers/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6 6. kubernetes master初始化123456查看kubeadm版本# rpm -q kubeadmkubeadm-1.13.4-0.x86_64执行命令初始化master# kubeadm init --kubernetes-version=&quot;1.13.4&quot; --pod-network-cidr=&quot;10.244.0.0/16&quot; --ignore-preflight-errors=Swap 初始化信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[init] Using Kubernetes version: v1.13.4[preflight] Running pre-flight checks [WARNING Swap]: running with swap on is not supported. Please disable swap [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06 [WARNING Service-Kubelet]: kubelet service is not enabled, please run &apos;systemctl enable kubelet.service&apos;[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;[certs] Generating &quot;front-proxy-ca&quot; certificate and key[certs] Generating &quot;front-proxy-client&quot; certificate and key[certs] Generating &quot;etcd/ca&quot; certificate and key[certs] Generating &quot;etcd/server&quot; certificate and key[certs] etcd/server serving cert is signed for DNS names [master.k8s.com localhost] and IPs [192.168.110.140 127.0.0.1 ::1][certs] Generating &quot;etcd/peer&quot; certificate and key[certs] etcd/peer serving cert is signed for DNS names [master.k8s.com localhost] and IPs [192.168.110.140 127.0.0.1 ::1][certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key[certs] Generating &quot;ca&quot; certificate and key[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key[certs] Generating &quot;apiserver&quot; certificate and key[certs] apiserver serving cert is signed for DNS names [master.k8s.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.110.140][certs] Generating &quot;sa&quot; key and public key[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s[apiclient] All control plane components are healthy after 24.503251 seconds[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.13&quot; in namespace kube-system with the configuration for the kubelets in the cluster[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;master.k8s.com&quot; as an annotation[mark-control-plane] Marking the node master.k8s.com as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;[mark-control-plane] Marking the node master.k8s.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: 0me0lv.xn1vhr6ub80qexyu[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.110.140:6443 --token 0me0lv.xn1vhr6ub80qexyu --discovery-token-ca-cert-hash sha256:700e4cb4ce01c0be5a40e8278ebf531847090b53d53b115ca3031bfb25532b0b 根据提示在master节点下执行以下命令123# mkdir -p $HOME/.kube# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config# sudo chown $(id -u):$(id -g) $HOME/.kube/config 7. kubernetes node01加入集群node01执行以下命令加入集群1#kubeadm join 192.168.110.140:6443 --token 0me0lv.xn1vhr6ub80qexyu --discovery-token-ca-cert-hash sha256:700e4cb4ce01c0be5a40e8278ebf531847090b53d53b115ca3031bfb25532b0b --ignore-preflight-errors=Swap 执行命令后提示信息1234567891011121314151617181920212223[preflight] Running pre-flight checks [WARNING Swap]: running with swap on is not supported. Please disable swap [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06 [WARNING Service-Kubelet]: kubelet service is not enabled, please run &apos;systemctl enable kubelet.service&apos;[discovery] Trying to connect to API Server &quot;192.168.110.140:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://192.168.110.140:6443&quot;[discovery] Requesting info from &quot;https://192.168.110.140:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;192.168.110.140:6443&quot;[discovery] Successfully established connection with API Server &quot;192.168.110.140:6443&quot;[join] Reading configuration from the cluster...[join] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.13&quot; ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Activating the kubelet service[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;node01.k8s.com&quot; as an annotationThis node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster. 8. kubernetes node02加入集群node02执行以下命令加入集群1#kubeadm join 192.168.110.140:6443 --token 0me0lv.xn1vhr6ub80qexyu --discovery-token-ca-cert-hash sha256:700e4cb4ce01c0be5a40e8278ebf531847090b53d53b115ca3031bfb25532b0b --ignore-preflight-errors=Swap 以上是配置好了kubernetes 一个master节点和两个node节点 9. 验证集群状态分别在3个节点上执行命令启动kubelet1# system start kubelet 在master节点上执行命令查看集群节点状态123456# kubectl get nodes得到以下信息NAME STATUS ROLES AGE VERSIONmaster.k8s.com Ready master 18m v1.13.4node01.k8s.com Ready &lt;none&gt; 11m v1.13.4node02.k8s.com Ready &lt;none&gt; 9m39s v1.13.4 在master节点上执行命令查看系统pod运行状态123456789101112131415# kubectl get pods -n kube-system得到以下信息NAME READY STATUS RESTARTS AGEcoredns-86c58d9df4-467w8 1/1 Running 0 23mcoredns-86c58d9df4-qrjx2 1/1 Running 0 23metcd-master.k8s.com 1/1 Running 0 22mkube-apiserver-master.k8s.com 1/1 Running 0 22mkube-controller-manager-master.k8s.com 1/1 Running 0 22mkube-flannel-ds-amd64-4hx5q 1/1 Running 0 5m1skube-flannel-ds-amd64-nhbqs 1/1 Running 0 5m1skube-flannel-ds-amd64-pf5dg 1/1 Running 0 5m1skube-proxy-g85lj 1/1 Running 0 14mkube-proxy-kb9l7 1/1 Running 0 23mkube-proxy-nczpr 1/1 Running 0 16mkube-scheduler-master.k8s.com 1/1 Running 0 22m 9. 错误排查在master节点上执行命令得到如下状态1234567891011121314151617# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster.k8s.com NotReady master 15m v1.13.4node01.k8s.com NotReady &lt;none&gt; 8m6s v1.13.4node02.k8s.com NotReady &lt;none&gt; 6m31s v1.13.4# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-86c58d9df4-467w8 0/1 Pending 0 17mcoredns-86c58d9df4-qrjx2 0/1 Pending 0 17metcd-master.k8s.com 1/1 Running 0 16mkube-apiserver-master.k8s.com 1/1 Running 0 16mkube-controller-manager-master.k8s.com 1/1 Running 0 16mkube-proxy-g85lj 1/1 Running 0 8m19skube-proxy-kb9l7 1/1 Running 0 17mkube-proxy-nczpr 1/1 Running 0 9m54skube-scheduler-master.k8s.com 1/1 Running 0 16m 以上说明网络没有配置好，需要我们安装网络，此时参考 4.11. 安装网络插件（flannel或calico）执行命令安装flannel1# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 执行命令后再次验证节点和pod状态 以上完整安装kubernetes 13.3.4 整个过程]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle11g DataGuard配置（一主二备）]]></title>
    <url>%2F2018%2F10%2F09%2Foracle_dataguard2%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 环境准备硬件环境：我们使用VM虚拟机来安装两个Linux虚拟机。虚拟机安装Oracle Linux 5.6 配置如下：内存：2GCPU：1个磁盘：20G 操作系统：Red Hat Enterprise Linux Server release 5.6 (Tikanga) oracle数据库：oracle 11.0.2.4 主机名 ip地址 db_name db_unique_name service_name master(主库) 192.168.110.66 orcl mdb orcl node01(备库) 192.168.110.88 orcl dg01 orcl node02(备库) 192.168.110.99 orcl dg02 orcl 需要在三台虚拟机上分别安装oracle11g软件，只安装软件，不用建立数据库。我们可以先在一台虚拟机上安装好oracle11g，然后复制出来两份虚拟机，然后修改相关参数变成一台新的数据库服务器。关于oracle11g数据库安装由于篇幅原因本文略过，本文主要是关于DataGuard配置。 master机器上编辑环境变量oracle用户登录，在oracle用户home目录下编辑环境变量（可根据实际路径填写）vi .bash_profile12345678910export ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0.4/db_1export ORACLE_SID=orclexport ORACLE_UNQNAME=mdbPATH=$ORACLE_HOME/bin:$PATH:$HOME/binLD_LIBARY_PATH=$ORACLE_HOME/lib:$LD_LIBARY_PATHDISPLAY=192.168.110.1:0.0export LANG=Cexport ORACLE_BASE ORACLE_HOME ORACLE_SID PATH LD_LIBARY_PATH DISPLAYumask 022 node01机器上编辑环境变量oracle用户登录，在oracle用户home目录下编辑环境变量（可根据实际路径填写）vi .bash_profile12345678910export ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0.4/db_1export ORACLE_SID=orclexport ORACLE_UNQNAME=dg01PATH=$ORACLE_HOME/bin:$PATH:$HOME/binLD_LIBARY_PATH=$ORACLE_HOME/lib:$LD_LIBARY_PATHDISPLAY=192.168.110.1:0.0export LANG=Cexport ORACLE_BASE ORACLE_HOME ORACLE_SID PATH LD_LIBARY_PATH DISPLAYumask 022 node02机器上编辑环境变量oracle用户登录，在oracle用户home目录下编辑环境变量（可根据实际路径填写）vi .bash_profile12345678910export ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0.4/db_1export ORACLE_SID=orclexport ORACLE_UNQNAME=dg02PATH=$ORACLE_HOME/bin:$PATH:$HOME/binLD_LIBARY_PATH=$ORACLE_HOME/lib:$LD_LIBARY_PATHDISPLAY=192.168.110.1:0.0export LANG=Cexport ORACLE_BASE ORACLE_HOME ORACLE_SID PATH LD_LIBARY_PATH DISPLAYumask 022 在master机器上执行 dbca 创建数据库数据库名orcl,数据库sys和system用户密码一样oracle（过程略过）。 要求node01和node02密码文件一致master机器上数据库建立好后，要求master和node01,node02机器上数据库密码一致，在master机器上执行命令(master和node01,node02数据库名一致可以这样操作，master和node01,node02数据库名不一致则密码文件在node01,node02机器上需要改名，密码文件规则orapw+sid)123$ cd $ORACLE_HOME/dbs$ scp orapworcl node01:/u01/app/oracle/product/11.2.0.4/db_1/dbs/orapworcl$ scp orapworcl node02:/u01/app/oracle/product/11.2.0.4/db_1/dbs/orapworcl master和node01和node02的tnsnames.ora文件一致master和node01两台机器的tnsnames.ora文件一致，内容如下(注意service_name)vi $ORACLE_HOME/network/admin/tnsnames.ora123456789101112131415161718192021222324252627mdb = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.66)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl)(UR=A) ) )dg01 = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.88)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl)(UR=A) ) )dg02 = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.99)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl)(UR=A) ) ) master机器上保存退出后可以复制到node01和node0212$ scp $ORACLE_HOME/network/admin/tnsnames.ora node01:$ORACLE_HOME/network/admin/tnsnames.ora$ scp $ORACLE_HOME/network/admin/tnsnames.ora node02:$ORACLE_HOME/network/admin/tnsnames.ora master的监听文件listener.ora做静态注册vi $ORACLE_HOME/network/admin/listener.ora1234567891011121314151617181920LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.66)(PORT = 1521)) (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) ) )SID_LIST_LISTENER =(SID_LIST= (SID_DESC= (SID_NAME= orcl) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) (SID_DESC= (SID_NAME= PLSExtProc) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) )ADR_BASE_LISTENER = /u01/app/oracle node01的监听文件listener.ora做静态注册vi $ORACLE_HOME/network/admin/listener.ora1234567891011121314151617181920LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.88)(PORT = 1521)) (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) ) )SID_LIST_LISTENER =(SID_LIST= (SID_DESC= (SID_NAME= orcl) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) (SID_DESC= (SID_NAME= PLSExtProc) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) )ADR_BASE_LISTENER = /u01/app/oracle node02的监听文件listener.ora做静态注册vi $ORACLE_HOME/network/admin/listener.ora1234567891011121314151617181920LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.99)(PORT = 1521)) (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) ) )SID_LIST_LISTENER =(SID_LIST= (SID_DESC= (SID_NAME= orcl) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) (SID_DESC= (SID_NAME= PLSExtProc) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) )ADR_BASE_LISTENER = /u01/app/oracle master和node01和node02启动监听服务（如果监听已经启动需要先关闭）12$ lsnrctl stop$ lsnrctl start master和node01和node02分别测试连接是否正常连接master node01 node02 分别测试连接master和node01和node021234567$ sqlplus /nolog SQL&gt; conn sys/oracle@mdb as sysdbaConnected.SQL&gt; conn sys/oracle@dg01 as sysdbaConnected to an idle instance.SQL&gt; conn sys/oracle@dg02 as sysdbaConnected to an idle instance. 修改master的参数修改master的参数（fal_server=对方 ,fal_client=自己, 值设置的是tns监听的名字，具体见下方）123456789101112SQL&gt; conn / as sysdbaSQL&gt; alter system set db_unique_name=&apos;mdb&apos; scope=spfile;SQL&gt; alter system set log_archive_config=&apos;dg_config=(mdb,dg01,dg02)&apos;;SQL&gt; alter system set log_archive_dest_2=&apos;service=dg01 async valid_for=(online_logfile,primary_role) db_unique_name=dg01&apos;;SQL&gt; alter system set log_archive_dest_3=&apos;service=dg02 async valid_for=(online_logfile,primary_role) db_unique_name=dg02&apos;;SQL&gt; alter system set fal_server=dg01,dg02;SQL&gt; alter system set fal_client=mdb;SQL&gt; alter system set standby_file_management=auto;SQL&gt; alter system set service_names=orcl;SQL&gt; create pfile from spfile;文件生成位置 /u01/app/oracle/product/11.2.0.4/db_1/dbs/initorcl.ora master机器上数据库开启强制logging和归档模式（归档模式需要数据库mount状态下修改）12345SQL&gt; shutdown immediateSQL&gt; startup mountSQL&gt; alter database archivelog;SQL&gt; alter database open;SQL&gt; alter database force logging; 查看master机器上数据库logfile信息确认logfile大小12345678910111213141516SQL&gt; col member for a50;SQL&gt; select group#,status,type,member from v$logfile order by 1; GROUP# STATUS TYPE MEMBER---------- ------- ------- -------------------------------------------------- 1 ONLINE /u01/app/oracle/oradata/orcl/redo01.log 2 ONLINE /u01/app/oracle/oradata/orcl/redo02.log 3 ONLINE /u01/app/oracle/oradata/orcl/redo03.logSQL&gt; select group#,bytes from v$log; GROUP# BYTES---------- ---------- 1 52428800 2 52428800 3 52428800 master机器上数据库增加STANDBY LOGFILE要求大小和logfile一致，组要求多一组1234SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo01.rdo&apos; size 52428800;SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo02.rdo&apos; size 52428800;SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo03.rdo&apos; size 52428800;SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo04.rdo&apos; size 52428800; 检查master和node01和node02文件夹创建对等目录要求两边目录一致包含子目录123456789在master和node01机器上分别执行 $ cd $ORACLE_BASE 主要检查目录admin checkpointsdiagfast_recovery_areaflash_recovery_areaoradata 在node01机器上执行建立目录12345678$ cd $ORACLE_BASE$ mkdir -p admin/orcl/adump$ mkdir -p admin/orcl/dpdump$ mkdir -p admin/orcl/pfile$ mkdir -p oradata/orcl$ mkdir -p fast_recovery_area/orcl$ mkdir -p fast_recovery_area/ORCL$ mkdir -p flash_recovery_area 在node02机器上执行建立目录12345678$ cd $ORACLE_BASE$ mkdir -p admin/orcl/adump$ mkdir -p admin/orcl/dpdump$ mkdir -p admin/orcl/pfile$ mkdir -p oradata/orcl$ mkdir -p fast_recovery_area/orcl$ mkdir -p fast_recovery_area/ORCL$ mkdir -p flash_recovery_area 准备node01的initnode01.ora文件1234master机器上执行（把master生成的pfile拷贝到node01）$ cd $ORACLE_HOME/dbs$ scp initorcl.ora node01:/u01/initnode01.ora$ scp initorcl.ora node02:/u01/initnode02.ora node01机器上打开文件 /u01/initnode01.ora 修改参数1234567*.fal_client=’dg01’*.fal_server=&apos;mdb,dg02&apos;*.log_archive_dest_2=&apos;service=mdb async valid_for=(online_logfile,primary_role) db_unique_name=mdb&apos;*.log_archive_dest_3=&apos;service=dg02 async valid_for=(online_logfile,primary_role) db_unique_name=dg02&apos;追加参数*.db_unique_name=&apos;dg01&apos;*.service_names=orcl; node02机器上打开文件 /u01/initnode02.ora 修改参数1234567*.fal_client=’dg02’*.fal_server=&apos;mdb,dg01&apos;*.log_archive_dest_2=&apos;service=mdb async valid_for=(online_logfile,primary_role) db_unique_name=mdb&apos;*.log_archive_dest_3=&apos;service=dg01 async valid_for=(online_logfile,primary_role) db_unique_name=dg01&apos;追加参数*.db_unique_name=&apos;dg02&apos;*.service_names=orcl; node01启动数据库到nomount使用initnode01.ora文件12$ sqlplus / as sysdbaSQL&gt; startup nomount pfile=/u01/initnode01.ora node01创建spfile文件(目的下次启动自动读取spfile启动)1SQL&gt; create spfile from pfile=&apos;/u01/initnode01.ora&apos;; node02启动数据库到nomount使用initnode02ora文件12$ sqlplus / as sysdbaSQL&gt; startup nomount pfile=/u01/initnode02.ora node02创建spfile文件(目的下次启动自动读取spfile启动)1SQL&gt; create spfile from pfile=&apos;/u01/initnode02.ora&apos;; master检查信息（注意：log_file_name_convert 参数值为 orcl, orcl，如果master和node01的数据库名称不一致，需要修改这个参数，db_file_name_convert同理）1234567891011121314SQL&gt; show parameter nameNAME TYPE VALUE------------------------------------ ----------- ------------------------------cell_offloadgroup_name stringdb_file_name_convert stringdb_name string orcldb_unique_name string mdbglobal_names boolean FALSEinstance_name string orcllock_name_space stringlog_file_name_convert stringprocessor_group_name stringservice_names string ORCL master机器上启动rman连接到mdb和dg01数据库执行12如果复制数据库时，备库的路径和原库一致，就需要加nofilenamecheck，不然会报错$ rman target sys/oracle@mdb auxiliary sys/oracle@dg01 master在rman执行duplicate命令1RMAN&gt; duplicate target database for standby from active database dorecover nofilenamecheck; 执行后查看信息（没有报错则正常执行，有报错需要排查相关错误后再次执行此命令） master机器上启动rman连接到mdb和dg02数据库执行12如果复制数据库时，备库的路径和原库一致，就需要加nofilenamecheck，不然会报错$ rman target sys/oracle@mdb auxiliary sys/oracle@dg02 master在rman执行duplicate命令1RMAN&gt; duplicate target database for standby from active database dorecover nofilenamecheck; 执行后查看信息（没有报错则正常执行，有报错需要排查相关错误后再次执行此命令） 执行成功后node01下查询数据库状态1234567SQL&gt; set lines 1000 pages 50000SQL&gt; col db_unique_name for a10SQL&gt; select db_unique_name ,open_mode,force_logging,database_role,switchover_status from v$database;DB_UNIQUE_ OPEN_MODE FOR DATABASE_ROLE SWITCHOVER_STATUS---------- -------------------- --- ---------------- --------------------dg01 MOUNTED YES PHYSICAL STANDBY NOT ALLOWED node01下执行命令，开始接收日志文件123SQL&gt; alter database recover managed standby database using current logfile disconnect;Database altered. 执行成功后node02下查询数据库状态1234567SQL&gt; set lines 1000 pages 50000SQL&gt; col db_unique_name for a10SQL&gt; select db_unique_name ,open_mode,force_logging,database_role,switchover_status from v$database;DB_UNIQUE_ OPEN_MODE FOR DATABASE_ROLE SWITCHOVER_STATUS---------- -------------------- --- ---------------- --------------------dg02 MOUNTED YES PHYSICAL STANDBY NOT ALLOWED node02下执行命令，开始接收日志文件123SQL&gt; alter database recover managed standby database using current logfile disconnect;Database altered. 在node01和node02机器上的数据库的归档目录下查看日志归档文件。 master机器上数据库执行日志切换123SQL&gt; alter system switch logfile;System altered. 再次node01和node02机器上的数据库的归档目录下查看日志归档文件。有新文件产生证明日志传过来了。 node01和node02目前运行在mount状态下，分别执行命令打开数据库123456先停止redo应用SQL&gt; alter database recover managed standby database cancel;打开node01数据库SQL&gt; alter database open;继续接收redo数据SQL&gt; alter database recover managed standby database using current logfile disconnect; 以上是Oracle11g DataGuard 一主两备配置的整个过程，我们可以在master数据库上某个用户下建立表插入数据，然后在node01和node02数据库执行查询查看效果（略过）。 Dataguard 开关顺序关闭：先主后备启动：先备后主]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
        <tag>Oracle11g</tag>
        <tag>DataGuard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle11g DataGuard配置（一主一备）]]></title>
    <url>%2F2018%2F10%2F08%2Foracle_dataguard%2F</url>
    <content type="text"><![CDATA[作者声明：本篇文章系本人依照真实部署过程原创，未经许可，谢绝转载。 环境准备硬件环境：我们使用VM虚拟机来安装两个Linux虚拟机。虚拟机安装Oracle Linux 5.6 配置如下：内存：2GCPU：1个磁盘：20G 操作系统：Red Hat Enterprise Linux Server release 5.6 (Tikanga) oracle数据库：oracle 11.0.2.4 主机名 ip地址 db_name db_unique_name service_name dg1(主库) 192.168.110.66 orcl dgs orcl dg2(备库) 192.168.110.88 orcl dgt orcl 需要在两台虚拟机上分别安装oracle11g软件，只安装软件，不用建立数据库。我们可以先在一台虚拟机上安装好oracle11g，然后复制出来一份虚拟机，然后修改相关参数变成一台新的数据库服务器。关于oracle11g数据库安装由于篇幅原因本文略过，本文主要是关于DataGuard配置。 dg1机器上编辑环境变量oracle用户登录，在oracle用户home目录下编辑环境变量（可根据实际路径填写）vi .bash_profile12345678910export ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0.4/db_1export ORACLE_SID=orclexport ORACLE_UNQNAME=dgsPATH=$ORACLE_HOME/bin:$PATH:$HOME/binLD_LIBARY_PATH=$ORACLE_HOME/lib:$LD_LIBARY_PATHDISPLAY=192.168.110.1:0.0export LANG=Cexport ORACLE_BASE ORACLE_HOME ORACLE_SID PATH LD_LIBARY_PATH DISPLAYumask 022 dg2机器上编辑环境变量oracle用户登录，在oracle用户home目录下编辑环境变量（可根据实际路径填写）vi .bash_profile12345678910export ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0.4/db_1export ORACLE_SID=orclexport ORACLE_UNQNAME=dgtPATH=$ORACLE_HOME/bin:$PATH:$HOME/binLD_LIBARY_PATH=$ORACLE_HOME/lib:$LD_LIBARY_PATHDISPLAY=192.168.110.1:0.0export LANG=Cexport ORACLE_BASE ORACLE_HOME ORACLE_SID PATH LD_LIBARY_PATH DISPLAYumask 022 在dg1上执行 dbca 创建数据库数据库名orcl,数据库sys和system用户密码一样oracle（过程略过）。 要求dg1和dg2密码文件一致dg1上数据库建立好后，要求dg1和dg2上数据库密码一致，在dg1上执行命令(dg1和dg2数据库名一致可以这样操作，dg1和dg2数据库名不一致则密码文件在dg2上需要改名，密码文件规则orapw+sid)12$ cd $ORACLE_HOME/dbs$ scp orapworcl dg2:/u01/app/oracle/product/11.2.0.4/db_1/dbs/orapworcl dg1和dg2的tnsnames.ora文件一致dg1和dg2两台机器的tnsnames.ora文件一致，内容如下(注意service_name)vi $ORACLE_HOME/network/admin/tnsnames.ora123456789101112131415161718dgs = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.66)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl)(UR=A) ) )dgt = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.88)(PORT = 1521)) ) (CONNECT_DATA = (SERVICE_NAME = orcl)(UR=A) ) ) dg1的监听文件listener.ora做静态注册vi $ORACLE_HOME/network/admin/listener.ora1234567891011121314151617181920LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.66)(PORT = 1521)) (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) ) )SID_LIST_LISTENER =(SID_LIST= (SID_DESC= (SID_NAME= orcl) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) (SID_DESC= (SID_NAME= PLSExtProc) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) )ADR_BASE_LISTENER = /u01/app/oracle dg2的监听文件listener.ora做静态注册vi $ORACLE_HOME/network/admin/listener.ora1234567891011121314151617181920LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.110.88)(PORT = 1521)) (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) ) )SID_LIST_LISTENER =(SID_LIST= (SID_DESC= (SID_NAME= orcl) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) (SID_DESC= (SID_NAME= PLSExtProc) (ORACLE_HOME=/u01/app/oracle/product/11.2.0.4/db_1) ) )ADR_BASE_LISTENER = /u01/app/oracle dg1和dg2启动监听服务（如果监听已经启动需要先关闭）12$ lsnrctl stop$ lsnrctl start dg1和dg2分别测试连接是否正常连接dg1 测试连接dgs和dgt123$ sqlplus /nolog SQL&gt; conn sys/oracle@dgs as sysdbaSQL&gt; conn sys/oracle@dgt as sysdba dg2 测试连接dgs和dgt123$ sqlplus /nolog SQL&gt; conn sys/oracle@dgs as sysdbaSQL&gt; conn sys/oracle@dgt as sysdba 修改dg1的参数修改dg1的参数（fal_server=对方 ,fal_client=自己, 值设置的是tns监听的名字，具体见下方）1234567891011SQL&gt; conn / as sysdbaSQL&gt; alter system set db_unique_name=&apos;dgs&apos; scope=spfile;SQL&gt; alter system set log_archive_config=&apos;dg_config=(dgs,dgt)&apos;;SQL&gt; alter system set log_archive_dest_2=&apos;service=dgt async valid_for=(online_logfile,primary_role) db_unique_name=dgt&apos;;SQL&gt; alter system set fal_server=dgt;SQL&gt; alter system set fal_client=dgs;SQL&gt; alter system set standby_file_management=auto;SQL&gt; alter system set service_names=orcl;SQL&gt; create pfile from spfile;文件生成位置 /u01/app/oracle/product/11.2.0.4/db_1/dbs/initorcl.ora dg1机器上数据库开启强制logging和归档模式（归档模式需要数据库mount状态下修改）12345SQL&gt; shutdown immediateSQL&gt; startup mountSQL&gt; alter database archivelog;SQL&gt; alter database open;SQL&gt; alter database force logging; 查看dg1下logfile信息确认logfile大小12345678910111213141516SQL&gt; col member for a50;SQL&gt; select group#,status,type,member from v$logfile order by 1; GROUP# STATUS TYPE MEMBER---------- ------- ------- -------------------------------------------------- 1 ONLINE /u01/app/oracle/oradata/orcl/redo01.log 2 ONLINE /u01/app/oracle/oradata/orcl/redo02.log 3 ONLINE /u01/app/oracle/oradata/orcl/redo03.logSQL&gt; select group#,bytes from v$log; GROUP# BYTES---------- ---------- 1 52428800 2 52428800 3 52428800 dg1下增加STANDBY LOGFILE要求大小和logfile一致，组要求多一组1234SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo01.rdo&apos; size 52428800;SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo02.rdo&apos; size 52428800;SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo03.rdo&apos; size 52428800;SQL&gt; alter database add standby logfile &apos;/u01/app/oracle/oradata/orcl/sredo04.rdo&apos; size 52428800; 检查dg1和dg2文件夹创建对等目录要求两边目录一致包含子目录123456789在dg1和dg2上分别执行 $ cd $ORACLE_BASE 主要检查目录admin checkpointsdiagfast_recovery_areaflash_recovery_areaoradata 在dg2上执行建立目录12345678$ cd $ORACLE_BASE$ mkdir -p admin/orcl/adump$ mkdir -p admin/orcl/dpdump$ mkdir -p admin/orcl/pfile$ mkdir -p oradata/orcl$ mkdir -p fast_recovery_area/orcl$ mkdir -p fast_recovery_area/ORCL$ mkdir -p flash_recovery_area 准备dg2的initdgt.ora文件123dg1上执行（把dg1生成的pfile拷贝到dg2）$ cd $ORACLE_HOME/dbs$ scp initorcl.ora dg2:/u01/initdgt.ora dg2上打开文件 /u01/initdgt.ora 修改参数123456789*.fal_client=’dgt’*.fal_server=&apos;dgs&apos;修改*.log_archive_dest_2=&apos;service=dgt async valid_for=(online_logfile,primary_role) db_unique_name=dgt&apos;修改为*.log_archive_dest_3=&apos;service=dgs async valid_for=(online_logfile,primary_role) db_unique_name=dgs&apos;追加参数*.db_unique_name=&apos;dgt&apos;*.service_names=orcl; dg2启动数据库到nomount使用initdgt.ora文件12$ sqlplus / as sysdbaSQL&gt; startup nomount pfile=/u01/initdgt.ora dg2创建spfile文件(目的下次启动自动读取spfile启动)1SQL&gt; create spfile from pfile=&apos;/u01/initdgt.ora&apos;; dg1检查信息（注意：log_file_name_convert 参数值为 orcl, orcl，如果dg1和dg2的数据库名称不一致，需要修改这个参数，db_file_name_convert同理）1234567891011121314SQL&gt; show parameter nameNAME TYPE VALUE------------------------------------ ----------- ------------------------------cell_offloadgroup_name stringdb_file_name_convert stringdb_name string orcldb_unique_name string dgsglobal_names boolean FALSEinstance_name string orcllock_name_space stringlog_file_name_convert string processor_group_name stringservice_names string ORCL dg1上启动rman连接到两个数据库执行1$ rman target sys/oracle@dgs auxiliary sys/oracle@dgt dg1在rman执行duplicate命令12如果复制数据库时，备库的路径和原库一致，就需要加nofilenamecheck，不然会报错RMAN&gt; duplicate target database for standby from active database dorecover nofilenamecheck; 执行后查看信息（没有报错则正常执行，有报错需要排查相关错误后再次执行此命令） 执行成功后dg2下查询数据库状态1234567SQL&gt; set lines 1000 pages 50000SQL&gt; col db_unique_name for a10SQL&gt; select db_unique_name ,open_mode,force_logging,database_role,switchover_status from v$database;DB_UNIQUE_ OPEN_MODE FOR DATABASE_ROLE SWITCHOVER_STATUS---------- -------------------- --- ---------------- --------------------dgt MOUNTED YES PHYSICAL STANDBY NOT ALLOWED dg2下执行命令，开始接收日志文件123SQL&gt; alter database recover managed standby database using current logfile disconnect;Database altered. dg2下查看日志归档情况 dg1下执行日志切换123SQL&gt; alter system switch logfile;System altered. 再次dg2下查看日志归档 有新文件产生证明日志传过来了 dg2目前运行在mount状态下，分别执行命令打开数据库123456先停止redo应用SQL&gt; alter database recover managed standby database cancel;打开dg2数据库SQL&gt; alter database open;继续接收redo数据SQL&gt; alter database recover managed standby database using current logfile disconnect; 以上是Oracle11g DataGuard 一主一备配置的整个过程，我们可以在dg1数据库上某个用户下建立表插入数据，然后在dg2数据库执行查询查看效果（略过）。 Dataguard 开关顺序关闭：先主后备启动：先备后主]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
        <tag>Oracle11g</tag>
        <tag>DataGuard</tag>
      </tags>
  </entry>
</search>
